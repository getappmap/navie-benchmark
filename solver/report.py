from argparse import ArgumentParser
import json
import os
import csv
from pathlib import Path
import sys
from typing import TypedDict


sys.path.append(
    str(Path(__file__).resolve().parents[1] / "submodules" / "navie-editor")
)
sys.path.append(str(Path(__file__).resolve().parents[1]))

from solver.workflow.solution import Solution
from solver.workflow.patch import Patch


class Prediction(TypedDict):
    instance_id: str
    repo: str
    base_commit: str
    patch: str
    test_patch: str
    created_at: str
    version: str
    model_patch: str
    model_name_or_path: str
    model_edit_test_file: str
    # These are available in the solution.json
    # model_test_patch: str
    # model_inverted_patch: str
    # These don't report very well, or we don't care
    # hints_text: str
    # FAIL_TO_PASS: List[str]
    # PASS_TO_PASS: List[str]
    # environment_setup_commit: str
    # problem_statement: str


class EvaluationReport(TypedDict):
    instance_id: str
    patch_is_None: bool
    patch_exists: bool
    patch_successfully_applied: bool
    resolved: bool


class Report:
    """
    Build a CSV report that combines evaluations results with the solution.json files
    generated by the solver.
    """

    def __init__(self, solve_data_dir, predictions_path, evaluation_logs_dir):
        self.solve_data_dir = solve_data_dir
        self.predictions_path = predictions_path
        self.evaluation_logs_dir = evaluation_logs_dir

    def generate(self, report_path: Path):
        def list_patch_files(patch: str) -> str:
            return " ".join(Patch(patch).list_files())

        solutions: dict[str, Solution] = {}
        for root, dirs, files in os.walk(self.solve_data_dir):
            for file in files:
                if file == "solution.json":
                    solution_data = json.load(open(Path(root) / file))
                    for patch_field in [
                        "code_patch",
                        "test_patch",
                        "test_inverted_patch",
                    ]:
                        if patch_field in solution_data:
                            solution_data[patch_field] = list_patch_files(
                                solution_data[patch_field]
                            )
                    if not solution_data.get("instance_id"):
                        root_tokens = root.split("/")
                        instance_id = root_tokens[1]
                        solution_data["instance_id"] = instance_id
                    assert "instance_id" in solution_data
                    solution = Solution(**solution_data)
                    if solution["instance_id"] in solutions.keys():
                        print(f"WARNING Duplicate solution: {solution['instance_id']} in {root}/{file}")
                    solutions[solution["instance_id"]] = solution
        print(f"Loaded {len(solutions)} solutions")

        predictions: dict[str, Prediction] = {}
        blank_predictions: dict[str, Prediction] = {}
        with open(self.predictions_path) as f:
            for line in f:
                data = json.loads(line)
                data = {
                    k: v for k, v in data.items() if k in Prediction.__annotations__
                }
                assert "instance_id" in data

                data["gold_patch"] = list_patch_files(data["patch"])
                data["gold_test_patch"] = list_patch_files(data["test_patch"])
                del data["patch"]
                del data["test_patch"]

                prediction = Prediction(**data)
                if prediction["model_patch"]:
                    if prediction["instance_id"] in predictions.keys():
                        print(f"WARNING Duplicate prediction: {prediction['instance_id']} in {self.predictions_path}")
                    else:
                        predictions[prediction["instance_id"]] = prediction
                else:
                    blank_predictions[prediction["instance_id"]] = prediction

        if blank_predictions:
            for instance_id in blank_predictions:
                if instance_id not in predictions:
                    print(f"WARNING Blank prediction: {instance_id}")
                    predictions[instance_id] = blank_predictions[instance_id]

        print(f"Loaded {len(predictions)} predictions")

        evaluation_reports: dict[str, EvaluationReport] = {}
        submitted_count = 0
        resolved_count = 0
        for root, dirs, files in os.walk(self.evaluation_logs_dir):
            for file in files:
                if file == "report.json":
                    data = json.load(open(Path(root) / file))
                    for key in data:
                        data_item = data[key]
                        data_item = {
                            k: v
                            for k, v in data_item.items()
                            if k in EvaluationReport.__annotations__
                        }
                        data_item["instance_id"] = key
                        assert "instance_id" in data_item
                        report = EvaluationReport(**data_item)
                        if report["instance_id"] in evaluation_reports.keys():
                            print(f"WARNING Duplicate evaluation report: {report['instance_id']} in {root}/{file}")

                        submitted_count += 1
                        if report["resolved"]:
                            resolved_count += 1

                        evaluation_reports[report["instance_id"]] = report
        print(f"Loaded {len(evaluation_reports)} evaluations")

        instance_ids = list(
            set(predictions.keys())
            | set(solutions.keys())
        )
        intersection_ids = (
            set(solutions.keys())
            & set(predictions.keys())
        )
        difference_ids = set(instance_ids) - intersection_ids
        if difference_ids:
            print(f"Instance IDs in one file but not the others: {" ".join(difference_ids)}")
            print(f"Solutions do not contain the following instance ids: {set(solutions.keys()) - set(intersection_ids)}")
            print(f"Predictions do not contain the following instance ids: {set(predictions.keys()) - set(intersection_ids)}")
            print(f"Evaluations do not contain the following instance ids: {set(evaluation_reports.keys()) - set(intersection_ids)}")

        instance_ids.sort()
        combined_data: list[dict] = []
        for instance_id in intersection_ids:
            solution = solutions.get(instance_id)
            prediction = predictions.get(instance_id)
            evaluation_report = evaluation_reports.get(instance_id)

            assert solution
            assert prediction

            # Collect all data into one record
            row = {}
            row.update(solution)
            row.update(prediction)
            if evaluation_report:
                row.update(evaluation_report)
            combined_data.append(row)

        # Print abbreviated results
        print(f"Total instances:     {len(predictions)}")
        print(f"Instances submitted: {submitted_count}")
        print(f"Instances resolved:  {resolved_count}")

        report_file = Path(report_path)
        with report_file.open("w") as f:
            unique_field_names = set()
            combined_field_names = list()
            for row in combined_data:
                for field_name in row.keys():
                    if field_name not in unique_field_names:
                        unique_field_names.add(field_name)
                        combined_field_names.append(field_name)

            writer = csv.DictWriter(f, fieldnames=combined_field_names)
            writer.writeheader()
            writer.writerows(combined_data)


if __name__ == "__main__":
    """
    Build a CSV report that combines evaluations results with the solution.json files
    generated by the solver.
    """
    parser = ArgumentParser(description="Print a console report and build a CSV report of the solver results")
    parser.add_argument(
        "--solve_data_dir",
        type=str,
        help="Directory containing the solver logs, including solution.json files",
        default="solve",
    )
    parser.add_argument(
        "--predictions_path",
        type=str,
        help="Path to the predictions file",
        default="predictions.jsonl",
    )
    parser.add_argument(
        "--evaluation_logs_dir",
        type=str,
        help="Directory containing the evaluation logs",
        default="logs/run_evaluation",
    )
    parser.add_argument(
        "--report_path",
        type=str,
        help="Path to write the report",
        default="report.csv",
    )

    args = parser.parse_args()

    solve_data_dir = Path(args.solve_data_dir)
    predictions_path = Path(args.predictions_path)
    evaluation_logs_dir = Path(args.evaluation_logs_dir)
    report_path = Path(args.report_path)
    report = Report(solve_data_dir, predictions_path, evaluation_logs_dir)
    report.generate(report_path)
