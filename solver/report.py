from argparse import ArgumentParser
import json
import os
import csv
from pathlib import Path
import sys
from typing import Optional, TypedDict


sys.path.append(
    str(Path(__file__).resolve().parents[1] / "submodules" / "navie-editor")
)
sys.path.append(str(Path(__file__).resolve().parents[1]))

from solver.workflow.solution import Solution
from solver.workflow.patch import Patch


class Prediction(TypedDict):
    instance_id: str
    repo: str
    base_commit: str
    patch: str
    test_patch: str
    created_at: str
    version: str
    model_patch: str
    model_name_or_path: str
    model_edit_test_file: str
    # These are available in the solution.json
    # model_test_patch: str
    # model_inverted_patch: str
    # These don't report very well, or we don't care
    # hints_text: str
    # FAIL_TO_PASS: List[str]
    # PASS_TO_PASS: List[str]
    # environment_setup_commit: str
    # problem_statement: str


class EvaluationReport(TypedDict):
    instance_id: str
    patch_is_None: bool
    patch_exists: bool
    patch_successfully_applied: bool
    resolved: bool


def list_patch_files(patch: str) -> str:
    return " ".join(Patch(patch).list_files())


class Report:
    """
    Build a CSV report that combines evaluations results with the solution.json files
    generated by the solver.
    """

    def __init__(
        self,
        solve_data_dir: Path,
        evaluation_logs_dir: Path,
        predictions_path: Optional[Path],
    ):
        self.solve_data_dir = solve_data_dir
        self.evaluation_logs_dir = evaluation_logs_dir
        self.predictions_path = predictions_path

    def generate(self, report_path: Path):
        def collect_solutions() -> dict[str, Solution]:
            solutions: dict[str, Solution] = {}
            solution_first_observed: dict[str, str] = {}

            solution_files = list(
                self.solve_data_dir.rglob("**/solve_code/**/solution.json")
            )
            if not solution_files:
                print(
                    f"No solution files found in {self.solve_data_dir}/**/solve_code. Using all solution.json files in {self.solve_data_dir}."
                )
                solution_files = self.solve_data_dir.rglob("**/solution.json")

            for file in solution_files:
                solution_data = json.load(open(file))
                for patch_field in [
                    "code_patch",
                    "test_patch",
                    "test_inverted_patch",
                ]:
                    if patch_field in solution_data:
                        solution_data[patch_field] = list_patch_files(
                            solution_data[patch_field]
                        )
                assert "instance_id" in solution_data
                solution = Solution(**solution_data)
                if solution["instance_id"] in solutions.keys():
                    print(
                        f"WARNING Duplicate solution: {solution['instance_id']} in {file}"
                    )
                    print(
                        f"First observed in {solution_first_observed[solution['instance_id']]}"
                    )
                else:
                    solution_first_observed[solution["instance_id"]] = str(file)
                solutions[solution["instance_id"]] = solution

            return solutions

        def collect_predictions() -> tuple[dict[str, Prediction], int]:
            prediction_file_count = 0
            predictions: dict[str, Prediction] = {}
            blank_predictions: dict[str, Prediction] = {}

            def collect_prediction(path: Path) -> None:
                nonlocal prediction_file_count
                prediction_file_count += 1
                with open(path) as f:
                    for line in f:
                        data = json.loads(line)
                        data = {
                            k: v
                            for k, v in data.items()
                            if k in Prediction.__annotations__
                        }
                        assert "instance_id" in data

                        data["gold_patch"] = list_patch_files(data["patch"])
                        data["gold_test_patch"] = list_patch_files(data["test_patch"])
                        del data["patch"]
                        del data["test_patch"]

                        prediction = Prediction(**data)
                        if prediction["model_patch"]:
                            if prediction["instance_id"] in predictions.keys():
                                print(
                                    f"WARNING Duplicate prediction: {prediction['instance_id']} in {path}"
                                )
                            else:
                                predictions[prediction["instance_id"]] = prediction
                        else:
                            blank_predictions[prediction["instance_id"]] = prediction

            if self.predictions_path and self.predictions_path.exists():
                print(f"Collecting predictions from {self.predictions_path}")
                collect_prediction(self.predictions_path)
            else:
                print(f"Collecting predictions from {self.solve_data_dir}")
                for file in self.solve_data_dir.rglob(
                    "**/evaluation/predictions.jsonl"
                ):
                    collect_prediction(Path(file))

            if blank_predictions:
                for instance_id in blank_predictions:
                    if instance_id not in predictions:
                        print(f"WARNING Blank prediction: {instance_id}")
                        predictions[instance_id] = blank_predictions[instance_id]

            return predictions, prediction_file_count

        def collect_evaluation_reports() -> (
            tuple[dict[str, EvaluationReport], int, int]
        ):
            evaluation_reports: dict[str, EvaluationReport] = {}
            submitted_count = 0
            resolved_count = 0
            for root, dirs, files in os.walk(self.evaluation_logs_dir):
                for file in files:
                    if file == "report.json":
                        data = json.load(open(Path(root) / file))
                        for key in data:
                            data_item = data[key]
                            data_item = {
                                k: v
                                for k, v in data_item.items()
                                if k in EvaluationReport.__annotations__
                            }
                            data_item["instance_id"] = key
                            assert "instance_id" in data_item
                            report = EvaluationReport(**data_item)
                            if report["instance_id"] in evaluation_reports.keys():
                                print(
                                    f"WARNING Duplicate evaluation report: {report['instance_id']} in {root}/{file}"
                                )

                            submitted_count += 1
                            if report["resolved"]:
                                resolved_count += 1

                            evaluation_reports[report["instance_id"]] = report

            return evaluation_reports, submitted_count, resolved_count

        solutions = collect_solutions()
        print(f"Loaded {len(solutions)} solutions")

        predictions, prediction_file_count = collect_predictions()
        print(
            f"Loaded {len(predictions)} predictions from {prediction_file_count} files"
        )

        evaluation_reports, submitted_count, resolved_count = (
            collect_evaluation_reports()
        )
        print(f"Loaded {len(evaluation_reports)} evaluations")

        instance_ids = list(set(predictions.keys()) | set(solutions.keys()))
        intersection_ids = set(solutions.keys()) & set(predictions.keys())
        difference_ids = set(instance_ids) - intersection_ids
        if difference_ids:
            instance_ids_do_not_contain = " ".join(difference_ids)
            print(
                f"Instance IDs in one file but not the others: {instance_ids_do_not_contain}"
            )
            solutions_do_not_contain = set(instance_ids) - set(solutions.keys())
            print(
                f"Solutions do not contain the following instance ids: {solutions_do_not_contain}"
            )
            predictions_do_not_contain = set(instance_ids) - set(predictions.keys())
            print(
                f"Predictions do not contain the following instance ids: {predictions_do_not_contain}"
            )
            evaluation_reports_do_not_contain = set(instance_ids) - set(
                evaluation_reports.keys()
            )
            print(
                f"Evaluations do not contain the following instance ids: {evaluation_reports_do_not_contain}"
            )

        instance_ids.sort()
        combined_data: list[dict] = []
        for instance_id in intersection_ids:
            solution = solutions.get(instance_id)
            prediction = predictions.get(instance_id)
            evaluation_report = evaluation_reports.get(instance_id)

            assert solution
            assert prediction

            # Collect all data into one record
            row = {}
            row.update(solution)
            row.update(prediction)
            if evaluation_report:
                row.update(evaluation_report)

            if "model_patch" in row:
                del row["model_patch"]
            combined_data.append(row)

        # Print abbreviated results
        print(f"Total instances:     {len(predictions)}")
        print(f"Instances submitted: {submitted_count}")
        print(f"Instances resolved:  {resolved_count}")

        report_file = Path(report_path)
        with report_file.open("w") as f:
            unique_field_names = set()
            combined_field_names = list()
            for row in combined_data:
                for field_name in row.keys():
                    if field_name not in unique_field_names:
                        unique_field_names.add(field_name)
                        combined_field_names.append(field_name)

            writer = csv.DictWriter(f, fieldnames=combined_field_names)
            writer.writeheader()
            writer.writerows(combined_data)


if __name__ == "__main__":
    """
    Build a CSV report that combines evaluations results with the solution.json files
    generated by the solver.
    """
    parser = ArgumentParser(
        description="Print a console report and build a CSV report of the solver results"
    )
    parser.add_argument(
        "--archive_dir",
        type=str,
        help="All-in-one directory containing the solver logs, evaluations, and predictions files",
    )
    parser.add_argument(
        "--solve_data_dir",
        type=str,
        help="Directory containing the solver logs, including solution.json files",
    )
    parser.add_argument(
        "--evaluation_logs_dir",
        type=str,
        help="Directory containing the evaluation logs",
    )
    parser.add_argument(
        "--predictions_path",
        type=str,
        help="Path to the predictions file",
    )
    parser.add_argument(
        "--report_path",
        type=str,
        help="Path to write the report",
        default="report.csv",
    )

    args = parser.parse_args()

    if args.archive_dir:
        if args.solve_data_dir or args.evaluation_logs_dir or args.predictions_path:
            print(
                "If --archive_dir is specified, do not specify --solve_data_dir, --evaluation_logs_dir, or --predictions_path"
            )
            sys.exit(1)

        solve_data_dir = Path(args.archive_dir)
        evaluation_logs_dir = Path(args.archive_dir)
        predictions_path = None
    else:
        if (
            not args.solve_data_dir
            or not args.evaluation_logs_dir
            or not args.predictions_path
        ):
            print(
                "If --archive_dir is not specified, --solve_data_dir, --evaluation_logs_dir, and --predictions_path must be specified"
            )
            sys.exit(1)

        solve_data_dir = Path(args.solve_data_dir)
        evaluation_logs_dir = Path(args.evaluation_logs_dir)
        predictions_path = Path(args.predictions_path)

    report_path = Path(args.report_path)
    report = Report(solve_data_dir, evaluation_logs_dir, predictions_path)
    report.generate(report_path)
