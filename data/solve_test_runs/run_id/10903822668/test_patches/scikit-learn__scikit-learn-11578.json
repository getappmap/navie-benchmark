{
  "edit_test_file": "sklearn/linear_model/tests/test_logistic.py",
  "test_patch": "diff --git a/sklearn/linear_model/tests/test_logistic_regression_cv_multiclass.py b/sklearn/linear_model/tests/test_logistic_regression_cv_multiclass.py\nnew file mode 100644\nindex 0000000..422d14a\n--- /dev/null\n+++ b/sklearn/linear_model/tests/test_logistic_regression_cv_multiclass.py\n@@ -0,0 +1,69 @@\n+import numpy as np\n+from sklearn import preprocessing, linear_model, utils\n+from sklearn.utils.testing import assert_almost_equal\n+\n+def test_logistic_regression_cv_multiclass():\n+    def ovr_approach(decision_function):\n+        probs = 1. / (1. + np.exp(-decision_function))\n+        probs = probs / probs.sum(axis=1).reshape((probs.shape[0], -1))\n+        return probs\n+\n+    def score_from_probs(probs, y_bin):\n+        return (y_bin * np.log(probs)).sum(axis=1).mean()\n+\n+    np.random.seed(seed=1234)\n+\n+    samples = 200\n+    features = 5\n+    folds = 10\n+\n+    # Use a \"probabilistic\" scorer\n+    scorer = 'neg_log_loss'\n+\n+    x = np.random.random(size=(samples, features))\n+    y = np.random.choice(['a', 'b', 'c'], size=samples)\n+\n+    test = np.random.choice(range(samples), size=int(samples / float(folds)), replace=False)\n+    train = [idx for idx in range(samples) if idx not in test]\n+\n+    # Binarize the labels for y[test]\n+    lb = preprocessing.label.LabelBinarizer()\n+    lb.fit(y[test])\n+    y_bin = lb.transform(y[test])\n+\n+    # What does _log_reg_scoring_path give us for the score?\n+    coefs, _, scores, _ = linear_model.logistic._log_reg_scoring_path(\n+        x, y, train, test, fit_intercept=True, scoring=scorer, multi_class='multinomial'\n+    )\n+\n+    # Choose a single C to look at, for simplicity\n+    c_index = 0\n+    coefs = coefs[c_index]\n+    scores = scores[c_index]\n+\n+    # Initialise a LogisticRegression() instance, as in the issue description\n+    existing_log_reg = linear_model.LogisticRegression(fit_intercept=True)\n+    existing_log_reg.coef_ = coefs[:, :-1]\n+    existing_log_reg.intercept_ = coefs[:, -1]\n+\n+    existing_dec_fn = existing_log_reg.decision_function(x[test])\n+    existing_probs_builtin = existing_log_reg.predict_proba(x[test])\n+\n+    # OvR approach\n+    existing_probs_ovr = ovr_approach(existing_dec_fn)\n+\n+    # multinomial approach\n+    existing_probs_multi = utils.extmath.softmax(existing_dec_fn)\n+\n+    # If we initialise our LogisticRegression() instance, with multi_class='multinomial'\n+    new_log_reg = linear_model.LogisticRegression(fit_intercept=True, multi_class='multinomial')\n+    new_log_reg.coef_ = coefs[:, :-1]\n+    new_log_reg.intercept_ = coefs[:, -1]\n+\n+    new_dec_fn = new_log_reg.decision_function(x[test])\n+    new_probs_builtin = new_log_reg.predict_proba(x[test])\n+\n+    # Assert that the issue is reproduced\n+    assert_almost_equal(scores, score_from_probs(existing_probs_ovr, y_bin))\n+    assert not np.allclose(existing_probs_builtin, existing_probs_multi)\n+    assert np.allclose(new_probs_builtin, existing_probs_multi)\n",
  "inverted_patch": null
}