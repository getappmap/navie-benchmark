{
  "edit_test_file": "sklearn/tests/test_multioutput.py",
  "test_patch": "diff --git a/sklearn/tests/test_logistic_regression_scorer_issue.py b/sklearn/tests/test_logistic_regression_scorer_issue.py\nnew file mode 100644\nindex 0000000..5ae786b\n--- /dev/null\n+++ b/sklearn/tests/test_logistic_regression_scorer_issue.py\n@@ -0,0 +1,61 @@\n+import numpy as np\n+from sklearn import preprocessing, linear_model, utils\n+from sklearn.utils.testing import assert_almost_equal\n+\n+def ovr_approach(decision_function):\n+    probs = 1. / (1. + np.exp(-decision_function))\n+    probs = probs / probs.sum(axis=1).reshape((probs.shape[0], -1))\n+    return probs\n+\n+def score_from_probs(probs, y_bin):\n+    return (y_bin * np.log(probs)).sum(axis=1).mean()\n+\n+def test_logistic_regression_scorer_issue():\n+    np.random.seed(seed=1234)\n+\n+    samples = 200\n+    features = 5\n+    folds = 10\n+\n+    scorer = 'neg_log_loss'\n+\n+    x = np.random.random(size=(samples, features))\n+    y = np.random.choice(['a', 'b', 'c'], size=samples)\n+\n+    test = np.random.choice(range(samples), size=int(samples / float(folds)), replace=False)\n+    train = [idx for idx in range(samples) if idx not in test]\n+\n+    lb = preprocessing.label.LabelBinarizer()\n+    lb.fit(y[test])\n+    y_bin = lb.transform(y[test])\n+\n+    coefs, _, scores, _ = linear_model.logistic._log_reg_scoring_path(\n+        x, y, train, test, fit_intercept=True, scoring=scorer, multi_class='multinomial'\n+    )\n+\n+    c_index = 0\n+    coefs = coefs[c_index]\n+    scores = scores[c_index]\n+\n+    existing_log_reg = linear_model.LogisticRegression(fit_intercept=True)\n+    existing_log_reg.coef_ = coefs[:, :-1]\n+    existing_log_reg.intercept_ = coefs[:, -1]\n+\n+    existing_dec_fn = existing_log_reg.decision_function(x[test])\n+    existing_probs_builtin = existing_log_reg.predict_proba(x[test])\n+    existing_probs_ovr = ovr_approach(existing_dec_fn)\n+    existing_probs_multi = utils.extmath.softmax(existing_dec_fn)\n+\n+    new_log_reg = linear_model.LogisticRegression(fit_intercept=True, multi_class='multinomial')\n+    new_log_reg.coef_ = coefs[:, :-1]\n+    new_log_reg.intercept_ = coefs[:, -1]\n+\n+    new_dec_fn = new_log_reg.decision_function(x[test])\n+    new_probs_builtin = new_log_reg.predict_proba(x[test])\n+    new_probs_ovr = ovr_approach(new_dec_fn)\n+    new_probs_multi = utils.extmath.softmax(new_dec_fn)\n+\n+    assert_almost_equal(scores, score_from_probs(existing_probs_ovr, y_bin))\n+    assert not (existing_probs_builtin == existing_probs_multi).any()\n+    assert not (new_probs_builtin == new_probs_ovr).all()\n+    assert (new_probs_builtin == new_probs_multi).any()\n\\ No newline at end of file\n",
  "inverted_patch": "diff --git a/sklearn/tests/test_logistic_regression_scorer_issue_inverted.py b/sklearn/tests/test_logistic_regression_scorer_issue_inverted.py\nnew file mode 100644\nindex 0000000..b7a4a19\n--- /dev/null\n+++ b/sklearn/tests/test_logistic_regression_scorer_issue_inverted.py\n@@ -0,0 +1,61 @@\n+import numpy as np\n+from sklearn import preprocessing, linear_model, utils\n+from sklearn.utils.testing import assert_almost_equal\n+\n+def ovr_approach(decision_function):\n+    probs = 1. / (1. + np.exp(-decision_function))\n+    probs = probs / probs.sum(axis=1).reshape((probs.shape[0], -1))\n+    return probs\n+\n+def score_from_probs(probs, y_bin):\n+    return (y_bin * np.log(probs)).sum(axis=1).mean()\n+\n+def test_logistic_regression_scorer_issue():\n+    np.random.seed(seed=1234)\n+\n+    samples = 200\n+    features = 5\n+    folds = 10\n+\n+    scorer = 'neg_log_loss'\n+\n+    x = np.random.random(size=(samples, features))\n+    y = np.random.choice(['a', 'b', 'c'], size=samples)\n+\n+    test = np.random.choice(range(samples), size=int(samples / float(folds)), replace=False)\n+    train = [idx for idx in range(samples) if idx not in test]\n+\n+    lb = preprocessing.label.LabelBinarizer()\n+    lb.fit(y[test])\n+    y_bin = lb.transform(y[test])\n+\n+    coefs, _, scores, _ = linear_model.logistic._log_reg_scoring_path(\n+        x, y, train, test, fit_intercept=True, scoring=scorer, multi_class='multinomial'\n+    )\n+\n+    c_index = 0\n+    coefs = coefs[c_index]\n+    scores = scores[c_index]\n+\n+    existing_log_reg = linear_model.LogisticRegression(fit_intercept=True)\n+    existing_log_reg.coef_ = coefs[:, :-1]\n+    existing_log_reg.intercept_ = coefs[:, -1]\n+\n+    existing_dec_fn = existing_log_reg.decision_function(x[test])\n+    existing_probs_builtin = existing_log_reg.predict_proba(x[test])\n+    existing_probs_ovr = ovr_approach(existing_dec_fn)\n+    existing_probs_multi = utils.extmath.softmax(existing_dec_fn)\n+\n+    new_log_reg = linear_model.LogisticRegression(fit_intercept=True, multi_class='multinomial')\n+    new_log_reg.coef_ = coefs[:, :-1]\n+    new_log_reg.intercept_ = coefs[:, -1]\n+\n+    new_dec_fn = new_log_reg.decision_function(x[test])\n+    new_probs_builtin = new_log_reg.predict_proba(x[test])\n+    new_probs_ovr = ovr_approach(new_dec_fn)\n+    new_probs_multi = utils.extmath.softmax(new_dec_fn)\n+\n+    assert_almost_equal(scores, score_from_probs(existing_probs_ovr, y_bin))\n+    assert not (existing_probs_builtin == existing_probs_multi).any()\n+    assert not (new_probs_builtin == new_probs_ovr).all()\n+    assert not (new_probs_builtin == new_probs_multi).any(), \"__BUG__HERE__\"\n\\ No newline at end of file\n"
}