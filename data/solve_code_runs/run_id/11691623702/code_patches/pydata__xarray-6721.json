{
  "instance_id": "pydata__xarray-6721",
  "edit_test_file": "xarray/tests/test_dataset.py",
  "code_patch": "diff --git a/xarray/core/variable.py b/xarray/core/variable.py\nindex 2d115ff..394f2f4 100644\n--- a/xarray/core/variable.py\n+++ b/xarray/core/variable.py\n@@ -336,7 +336,7 @@ class Variable(AbstractArray, NdimSizeLenMixin, VariableArithmetic):\n \n     @property\n     def data(self):\n-        if is_duck_array(self._data):\n+        if is_duck_array(self._data) or hasattr(self._data, \"chunks\"):\n             return self._data\n         else:\n             return self.values\n@@ -987,8 +987,8 @@ class Variable(AbstractArray, NdimSizeLenMixin, VariableArithmetic):\n     @property\n     def chunks(self) -> tuple[tuple[int, ...], ...] | None:\n         \"\"\"\n-        Tuple of block lengths for this dataarray's data, in order of dimensions, or None if\n-        the underlying data is not a dask array.\n+        Tuple of block lengths for this variable's data, in order of dimensions, or None if\n+        the underlying data is not a chunked array (dask or zarr).\n \n         See Also\n         --------\n@@ -1014,8 +1014,10 @@ class Variable(AbstractArray, NdimSizeLenMixin, VariableArithmetic):\n         Variable.chunks\n         xarray.unify_chunks\n         \"\"\"\n-        if hasattr(self._data, \"chunks\"):\n-            return Frozen({dim: c for dim, c in zip(self.dims, self.data.chunks)})\n+        chunks = self.chunks\n+        if chunks is not None:\n+            # Convert tuple of chunk shapes to mapping of dims to chunk shapes\n+            return Frozen({dim: c for dim, c in zip(self.dims, chunks)})\n         else:\n             return {}\n \n",
  "test_patch": "diff --git a/xarray/tests/test_zarr_chunks.py b/xarray/tests/test_zarr_chunks.py\nnew file mode 100644\nindex 0000000..c40a247\n--- /dev/null\n+++ b/xarray/tests/test_zarr_chunks.py\n@@ -0,0 +1,39 @@\n+import pytest\n+import xarray as xr\n+import numpy as np\n+from xarray.testing import assert_identical\n+import zarr\n+import tempfile\n+import os\n+\n+\n+class TestZarrChunks:\n+    def test_chunks_loads_data(self):\n+        \"\"\"Test that accessing .chunks on a zarr-backed dataset loads data into memory\"\"\"\n+        \n+        # Create a large dataset that would be noticeable if loaded into memory\n+        shape = (1000, 1000)\n+        chunks = (100, 100)\n+        \n+        # Create some test data and save it to a temporary zarr store\n+        data = np.random.rand(*shape)\n+        ds = xr.Dataset({\"var\": ((\"x\", \"y\"), data)})\n+        \n+        with tempfile.TemporaryDirectory() as tmpdir:\n+            filename = os.path.join(tmpdir, \"test.zarr\") \n+            ds.to_zarr(filename)\n+            \n+            # Open the dataset with zarr backend\n+            ds_zarr = xr.open_dataset(filename, engine=\"zarr\")\n+            \n+            # Access chunks property - this should trigger loading\n+            # Use a context manager to catch memory allocations\n+            # This is a simple way to detect if data is being loaded\n+            # A more robust test would use a memory profiler\n+            try:\n+                chunks = ds_zarr.chunks\n+                # If we get here, the data was loaded into memory\n+                assert True\n+            except MemoryError:\n+                # We shouldn't get here - if we do, the test fails\n+                assert False, \"Data was not loaded into memory as expected\"\n",
  "test_inverted_patch": "diff --git a/xarray/tests/test_zarr_chunks_inverted.py b/xarray/tests/test_zarr_chunks_inverted.py\nnew file mode 100644\nindex 0000000..5b10c45\n--- /dev/null\n+++ b/xarray/tests/test_zarr_chunks_inverted.py\n@@ -0,0 +1,49 @@\n+import pytest\n+import xarray as xr\n+import numpy as np\n+from xarray.testing import assert_identical\n+import zarr\n+import tempfile\n+import os\n+import sys\n+\n+\n+class TestZarrChunks:\n+    def test_chunks_loads_data(self):\n+        \"\"\"Test that accessing .chunks on a zarr-backed dataset should NOT load data into memory\"\"\"\n+        \n+        # Create a large dataset that would be noticeable if loaded into memory\n+        shape = (1000, 1000)\n+        chunks = (100, 100)\n+        \n+        # Create some test data and save it to a temporary zarr store\n+        data = np.random.rand(*shape)\n+        ds = xr.Dataset({\"var\": ((\"x\", \"y\"), data)})\n+        \n+        with tempfile.TemporaryDirectory() as tmpdir:\n+            filename = os.path.join(tmpdir, \"test.zarr\") \n+            ds.to_zarr(filename)\n+            \n+            # Open the dataset with zarr backend\n+            ds_zarr = xr.open_dataset(filename, engine=\"zarr\")\n+            \n+            # Track memory usage before accessing chunks\n+            before_mem = sys.getsizeof(ds_zarr)\n+            \n+            # Access chunks property\n+            chunks = ds_zarr.chunks\n+            \n+            # Track memory usage after accessing chunks\n+            after_mem = sys.getsizeof(ds_zarr)\n+            \n+            # Calculate memory difference\n+            mem_diff = after_mem - before_mem\n+            \n+            # If memory usage increased significantly (more than 1MB),\n+            # it indicates the data was loaded into memory\n+            if mem_diff > 1024 * 1024:  # 1MB threshold\n+                raise AssertionError(\"__BUG__HERE__\")\n+            \n+            # Verify that the dataset is still lazy-loaded\n+            assert not ds_zarr['var'].variable._data.is_loaded(), \\\n+                   \"__BUG__HERE__\"\n\\ No newline at end of file\n",
  "num_sent_chars": 3583958,
  "num_received_chars": 46235,
  "elapsed_time": 1151.2705883979797,
  "lint_repair_count": 0,
  "test_generation_attempts": 0,
  "code_generation_attempts": 9,
  "pass_to_pass": true,
  "pass_to_fail": false,
  "fail_to_pass": false,
  "code_patch_score": 1,
  "appmap_data_test_status": null,
  "appmap_data_file_count": null,
  "appmap_data_context_size": null
}