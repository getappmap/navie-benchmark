{
  "instance_id": "scikit-learn__scikit-learn-13142",
  "edit_test_file": "sklearn/mixture/tests/test_gaussian_mixture.py",
  "code_patch": "diff --git a/sklearn/mixture/base.py b/sklearn/mixture/base.py\nindex bd34333..4fa4a42 100644\n--- a/sklearn/mixture/base.py\n+++ b/sklearn/mixture/base.py\n@@ -257,23 +257,23 @@ class BaseMixture(DensityMixin, BaseEstimator, metaclass=ABCMeta):\n                 best_params = self._get_parameters()\n                 best_n_iter = n_iter\n \n-        # Always do a final e-step to guarantee that the labels returned by\n-        # fit_predict(X) are always consistent with fit(X).predict(X)\n-        # for any value of max_iter and tol (and any random_state).\n-        _, log_resp = self._e_step(X)\n-\n-        if not self.converged_:\n-            warnings.warn('Initialization %d did not converge. '\n-                          'Try different init parameters, '\n-                          'or increase max_iter, tol '\n-                          'or check for degenerate data.'\n-                          % (init + 1), ConvergenceWarning)\n-\n-        self._set_parameters(best_params)\n-        self.n_iter_ = best_n_iter\n-        self.lower_bound_ = max_lower_bound\n-\n-        return log_resp.argmax(axis=1)\n+            # Always do a final e-step to guarantee that the labels returned by\n+            # fit_predict(X) are always consistent with fit(X).predict(X)\n+            # for any value of max_iter and tol (and any random_state).\n+            _, log_resp = self._e_step(X)\n+\n+            self._set_parameters(best_params)\n+            self.n_iter_ = best_n_iter\n+            self.lower_bound_ = max_lower_bound\n+\n+            if not self.converged_:\n+                warnings.warn('Initialization %d did not converge. '\n+                              'Try different init parameters, '\n+                              'or increase max_iter, tol '\n+                              'or check for degenerate data.'\n+                              % (init + 1), ConvergenceWarning)\n+\n+            return log_resp.argmax(axis=1)\n \n     def _e_step(self, X):\n         \"\"\"E step.\n",
  "test_patch": "diff --git a/sklearn/mixture/tests/test_gaussian_mixture_n_init.py b/sklearn/mixture/tests/test_gaussian_mixture_n_init.py\nnew file mode 100644\nindex 0000000..ffdbcc7\n--- /dev/null\n+++ b/sklearn/mixture/tests/test_gaussian_mixture_n_init.py\n@@ -0,0 +1,25 @@\n+# sklearn/mixture/tests/test_gaussian_mixture_n_init.py\n+\n+import numpy as np\n+from sklearn.mixture import GaussianMixture\n+from sklearn.utils.testing import assert_array_equal\n+import warnings\n+from sklearn.exceptions import ConvergenceWarning\n+\n+def test_gaussian_mixture_predict_fit_predict_discrepancy():\n+    # Suppress convergence warnings for this test\n+    with warnings.catch_warnings():\n+        warnings.simplefilter(\"ignore\", ConvergenceWarning)\n+\n+        # Generate random data\n+        X = np.random.randn(1000, 5)\n+\n+        # Test with n_init=5\n+        gm = GaussianMixture(n_components=5, n_init=5)\n+        c1 = gm.fit_predict(X)\n+        c2 = gm.predict(X)\n+        try:\n+            assert_array_equal(c1, c2)\n+        except AssertionError as e:\n+            print(\"Discrepancy detected between fit_predict and predict with n_init=5\")\n+            raise e\n",
  "test_inverted_patch": "diff --git a/sklearn/mixture/tests/test_gaussian_mixture_n_init_inverted.py b/sklearn/mixture/tests/test_gaussian_mixture_n_init_inverted.py\nnew file mode 100644\nindex 0000000..ee83972\n--- /dev/null\n+++ b/sklearn/mixture/tests/test_gaussian_mixture_n_init_inverted.py\n@@ -0,0 +1,24 @@\n+# sklearn/mixture/tests/test_gaussian_mixture_n_init.py\n+\n+import numpy as np\n+from sklearn.mixture import GaussianMixture\n+from sklearn.utils.testing import assert_array_equal\n+import warnings\n+from sklearn.exceptions import ConvergenceWarning\n+\n+def test_gaussian_mixture_predict_fit_predict_discrepancy():\n+    # Suppress convergence warnings for this test\n+    with warnings.catch_warnings():\n+        warnings.simplefilter(\"ignore\", ConvergenceWarning)\n+\n+        # Generate random data\n+        X = np.random.randn(1000, 5)\n+\n+        # Test with n_init=5\n+        gm = GaussianMixture(n_components=5, n_init=5)\n+        c1 = gm.fit_predict(X)\n+        c2 = gm.predict(X)\n+        try:\n+            assert_array_equal(c1, c2)\n+        except AssertionError:\n+            raise AssertionError(\"__BUG__HERE__\")\n",
  "num_sent_chars": 3046391,
  "num_received_chars": 44393,
  "elapsed_time": 331.4615659713745,
  "lint_repair_count": 3,
  "test_generation_attempts": 0,
  "code_generation_attempts": 6,
  "pass_to_pass": true,
  "pass_to_fail": false,
  "fail_to_pass": true,
  "code_patch_score": 2,
  "appmap_data_test_status": null,
  "appmap_data_file_count": null,
  "appmap_data_context_size": null
}