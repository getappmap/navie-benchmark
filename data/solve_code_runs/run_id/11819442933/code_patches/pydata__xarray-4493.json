{
  "instance_id": "pydata__xarray-4493",
  "edit_test_file": "xarray/tests/test_distributed.py",
  "code_patch": "diff --git a/xarray/core/dataset.py b/xarray/core/dataset.py\nindex 8376b48..2a81e24 100644\n--- a/xarray/core/dataset.py\n+++ b/xarray/core/dataset.py\n@@ -1046,7 +1046,16 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):\n         \"\"\"\n         if inplace:\n             if variables is not None:\n-                self._variables = variables\n+                # Preserve dask arrays and their chunking\n+                new_vars = {}\n+                for k, v in variables.items():\n+                    if k in self._variables and is_duck_dask_array(self._variables[k].data):\n+                        # Keep original dask array if dimensions match\n+                        if v.dims == self._variables[k].dims:\n+                            new_vars[k] = self._variables[k]\n+                            continue\n+                    new_vars[k] = v\n+                self._variables = new_vars\n             if coord_names is not None:\n                 self._coord_names = coord_names\n             if dims is not None:\n@@ -1061,6 +1070,17 @@ class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):\n         else:\n             if variables is None:\n                 variables = self._variables.copy()\n+            else:\n+                # Preserve dask arrays and their chunking in new dataset\n+                new_vars = {}\n+                for k, v in variables.items():\n+                    if k in self._variables and is_duck_dask_array(self._variables[k].data):\n+                        # Keep original dask array if dimensions match\n+                        if v.dims == self._variables[k].dims:\n+                            new_vars[k] = self._variables[k]\n+                            continue\n+                    new_vars[k] = v\n+                variables = new_vars\n             if coord_names is None:\n                 coord_names = self._coord_names.copy()\n             if dims is None:\n",
  "code_files": null,
  "test_patch": "diff --git a/xarray/tests/test_dataset_chunking.py b/xarray/tests/test_dataset_chunking.py\nnew file mode 100644\nindex 0000000..e860365\n--- /dev/null\n+++ b/xarray/tests/test_dataset_chunking.py\n@@ -0,0 +1,30 @@\n+import numpy as np\n+import pytest\n+\n+import xarray as xr\n+from xarray.tests import requires_dask\n+\n+dask = pytest.importorskip(\"dask\")\n+da = pytest.importorskip(\"dask.array\")\n+\n+\n+@requires_dask\n+def test_update_loses_chunks():\n+    # Create a chunked DataArray\n+    foo = xr.DataArray(np.random.randn(3, 3), dims=(\"x\", \"y\")).chunk()\n+    ds = xr.Dataset({\"foo\": foo, \"bar\": (\"x\", [1, 2, 3])})\n+    \n+    # Verify initial state is chunked\n+    assert isinstance(ds.foo.data, da.Array)\n+    \n+    # Create update dictionary with sliced data\n+    update_dict = {\"foo\": ((\"x\", \"y\"), ds.foo[1:, :]), \"bar\": (\"x\", ds.bar[1:])}\n+    \n+    # Verify the data in update_dict is still chunked\n+    assert isinstance(update_dict[\"foo\"][1].data, da.Array)\n+    \n+    # Perform the update\n+    ds.update(update_dict)\n+    \n+    # Verify that chunking was lost after update\n+    assert isinstance(ds.foo.data, np.ndarray)\n\\ No newline at end of file\n",
  "test_inverted_patch": "diff --git a/xarray/tests/test_dataset_chunking_inverted.py b/xarray/tests/test_dataset_chunking_inverted.py\nnew file mode 100644\nindex 0000000..68ffe21\n--- /dev/null\n+++ b/xarray/tests/test_dataset_chunking_inverted.py\n@@ -0,0 +1,31 @@\n+import numpy as np\n+import pytest\n+\n+import xarray as xr\n+from xarray.tests import requires_dask\n+\n+dask = pytest.importorskip(\"dask\")\n+da = pytest.importorskip(\"dask.array\")\n+\n+\n+@requires_dask\n+def test_update_should_preserve_chunks():\n+    # Create a chunked DataArray\n+    foo = xr.DataArray(np.random.randn(3, 3), dims=(\"x\", \"y\")).chunk()\n+    ds = xr.Dataset({\"foo\": foo, \"bar\": (\"x\", [1, 2, 3])})\n+    \n+    # Verify initial state is chunked\n+    assert isinstance(ds.foo.data, da.Array)\n+    \n+    # Create update dictionary with sliced data\n+    update_dict = {\"foo\": ((\"x\", \"y\"), ds.foo[1:, :]), \"bar\": (\"x\", ds.bar[1:])}\n+    \n+    # Verify the data in update_dict is still chunked\n+    assert isinstance(update_dict[\"foo\"][1].data, da.Array)\n+    \n+    # Perform the update\n+    ds.update(update_dict)\n+    \n+    # This should now fail when chunking is lost\n+    if not isinstance(ds.foo.data, da.Array):\n+        raise AssertionError(\"__BUG__HERE__\")\n\\ No newline at end of file\n",
  "num_sent_chars": 2118834,
  "num_received_chars": 26550,
  "elapsed_time": 455.2662150859833,
  "lint_repair_count": 0,
  "test_generation_attempts": 1,
  "code_generation_attempts": 2,
  "pass_to_pass": true,
  "pass_to_fail": true,
  "fail_to_pass": true,
  "code_patch_score": 3,
  "appmap_data_test_status": null,
  "appmap_data_file_count": null,
  "appmap_data_context_size": null
}