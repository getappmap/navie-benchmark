{
  "instance_id": "pytest-dev__pytest-7373",
  "edit_test_file": "testing/test_skipping.py",
  "code_patch": "diff --git a/src/_pytest/mark/evaluate.py b/src/_pytest/mark/evaluate.py\nindex 7591916..6744058 100644\n--- a/src/_pytest/mark/evaluate.py\n+++ b/src/_pytest/mark/evaluate.py\n@@ -18,17 +18,12 @@ from _pytest.store import StoreKey\n evalcache_key = StoreKey[Dict[str, Any]]()\n \n \n-def cached_eval(config: Config, expr: str, d: Dict[str, object]) -> Any:\n-    default = {}  # type: Dict[str, object]\n-    evalcache = config._store.setdefault(evalcache_key, default)\n-    try:\n-        return evalcache[expr]\n-    except KeyError:\n-        import _pytest._code\n+def uncached_eval(config: Config, expr: str, d: Dict[str, object]) -> Any:\n+    import _pytest._code\n \n-        exprcode = _pytest._code.compile(expr, mode=\"eval\")\n-        evalcache[expr] = x = eval(exprcode, d)\n-        return x\n+    exprcode = _pytest._code.compile(expr, mode=\"eval\")\n+    x = eval(exprcode, d)\n+    return x\n \n \n class MarkEvaluator:\n@@ -98,7 +93,7 @@ class MarkEvaluator:\n                     self.expr = expr\n                     if isinstance(expr, str):\n                         d = self._getglobals()\n-                        result = cached_eval(self.item.config, expr, d)\n+                        result = uncached_eval(self.item.config, expr, d)\n                     else:\n                         if \"reason\" not in mark.kwargs:\n                             # XXX better be checked at collection time\n",
  "code_files": null,
  "test_patch": "diff --git a/testing/test_mark_cache_eval.py b/testing/test_mark_cache_eval.py\nnew file mode 100644\nindex 0000000..1d2b28e\n--- /dev/null\n+++ b/testing/test_mark_cache_eval.py\n@@ -0,0 +1,38 @@\n+import pytest\n+import textwrap\n+\n+\n+def test_skipif_cache_evaluation(testdir):\n+    \"\"\"\n+    Test that skipif string condition evaluation is not incorrectly cached across modules.\n+    \"\"\"\n+    # Create first test module where skip=True\n+    testdir.makepyfile(\n+        test_module1=\"\"\"\n+        import pytest\n+        skip = True\n+        @pytest.mark.skipif(\"skip\")\n+        def test_should_skip():\n+            assert False\n+        \"\"\"\n+    )\n+\n+    # Create second test module where skip=False\n+    testdir.makepyfile(\n+        test_module2=\"\"\"\n+        import pytest\n+        skip = False\n+        @pytest.mark.skipif(\"skip\")\n+        def test_should_not_skip():\n+            assert False\n+        \"\"\"\n+    )\n+\n+    # Run both test modules\n+    result = testdir.runpytest(\"test_module1.py\", \"test_module2.py\")\n+\n+    # The first test should be skipped (1 skipped)\n+    # The second test should run and fail (1 failed)\n+    # But due to the caching bug, both tests are skipped (2 skipped)\n+    result.stdout.fnmatch_lines([\"*2 skipped*\"])\n+    assert result.ret == 0\n\\ No newline at end of file\n",
  "test_inverted_patch": "diff --git a/testing/test_mark_cache_eval_inverted.py b/testing/test_mark_cache_eval_inverted.py\nnew file mode 100644\nindex 0000000..c5864a3\n--- /dev/null\n+++ b/testing/test_mark_cache_eval_inverted.py\n@@ -0,0 +1,40 @@\n+def test_skipif_cache_evaluation(testdir):\n+    \"\"\"\n+    Test that skipif string condition evaluation is not incorrectly cached across modules.\n+    This test will FAIL when the caching bug is present.\n+    \"\"\"\n+    # Create first test module where skip=True\n+    testdir.makepyfile(\n+        test_module1=\"\"\"\n+        import pytest\n+        skip = True\n+        @pytest.mark.skipif(\"skip\")\n+        def test_should_skip():\n+            assert False\n+        \"\"\"\n+    )\n+\n+    # Create second test module where skip=False\n+    testdir.makepyfile(\n+        test_module2=\"\"\"\n+        import pytest\n+        skip = False\n+        @pytest.mark.skipif(\"skip\")\n+        def test_should_not_skip():\n+            assert False\n+        \"\"\"\n+    )\n+\n+    # Run both test modules\n+    result = testdir.runpytest(\"test_module1.py\", \"test_module2.py\")\n+\n+    # Check if both tests were skipped (which indicates the bug)\n+    skipped_tests = len([line for line in result.stdout.lines if \"skipped\" in line])\n+    if skipped_tests == 2:\n+        raise AssertionError(\"__BUG__HERE__\")\n+\n+    # The correct behavior should be:\n+    # - test_module1: 1 skipped\n+    # - test_module2: 1 failed\n+    result.stdout.fnmatch_lines([\"*1 failed, 1 skipped*\"])\n+    assert result.ret == 1  # Should be 1 because one test should fail\n\\ No newline at end of file\n",
  "num_sent_chars": 1185615,
  "num_received_chars": 13776,
  "elapsed_time": 211.69762539863586,
  "lint_repair_count": 0,
  "test_generation_attempts": 1,
  "code_generation_attempts": 1,
  "pass_to_pass": true,
  "pass_to_fail": true,
  "fail_to_pass": true,
  "code_patch_score": 3,
  "appmap_data_test_status": null,
  "appmap_data_file_count": null,
  "appmap_data_context_size": null
}