{
  "instance_id": "scikit-learn__scikit-learn-13142",
  "edit_test_file": "sklearn/mixture/tests/test_mixture.py",
  "code_patch": "diff --git a/sklearn/mixture/base.py b/sklearn/mixture/base.py\nindex bd34333..4e3acbb 100644\n--- a/sklearn/mixture/base.py\n+++ b/sklearn/mixture/base.py\n@@ -178,6 +178,9 @@ class BaseMixture(DensityMixin, BaseEstimator, metaclass=ABCMeta):\n         initialization is performed upon the first call. Upon consecutive\n         calls, training starts where it left off.\n \n+        After fitting, a final E-step is performed to ensure that the model's\n+        state is consistent for subsequent predictions.\n+\n         Parameters\n         ----------\n         X : array-like, shape (n_samples, n_features)\n@@ -257,23 +260,21 @@ class BaseMixture(DensityMixin, BaseEstimator, metaclass=ABCMeta):\n                 best_params = self._get_parameters()\n                 best_n_iter = n_iter\n \n-        # Always do a final e-step to guarantee that the labels returned by\n-        # fit_predict(X) are always consistent with fit(X).predict(X)\n-        # for any value of max_iter and tol (and any random_state).\n-        _, log_resp = self._e_step(X)\n+            if not self.converged_:\n+                warnings.warn('Initialization %d did not converge. '\n+                              'Try different init parameters, '\n+                              'or increase max_iter, tol '\n+                              'or check for degenerate data.'\n+                              % (init + 1), ConvergenceWarning)\n \n-        if not self.converged_:\n-            warnings.warn('Initialization %d did not converge. '\n-                          'Try different init parameters, '\n-                          'or increase max_iter, tol '\n-                          'or check for degenerate data.'\n-                          % (init + 1), ConvergenceWarning)\n+            self._set_parameters(best_params)\n+            self.n_iter_ = best_n_iter\n+            self.lower_bound_ = max_lower_bound\n \n-        self._set_parameters(best_params)\n-        self.n_iter_ = best_n_iter\n-        self.lower_bound_ = max_lower_bound\n+            # Perform a final E-step with the best parameters to ensure consistency\n+            _, log_resp = self._e_step(X)\n \n-        return log_resp.argmax(axis=1)\n+            return log_resp.argmax(axis=1)\n \n     def _e_step(self, X):\n         \"\"\"E step.\n",
  "test_patch": "diff --git a/sklearn/mixture/tests/test_gaussian_mixture_n_init.py b/sklearn/mixture/tests/test_gaussian_mixture_n_init.py\nnew file mode 100644\nindex 0000000..3a04d7f\n--- /dev/null\n+++ b/sklearn/mixture/tests/test_gaussian_mixture_n_init.py\n@@ -0,0 +1,23 @@\n+import numpy as np\n+from sklearn.mixture import GaussianMixture\n+from sklearn.utils.testing import assert_array_equal\n+import pytest\n+\n+def test_gaussian_mixture_fit_predict_disagreement():\n+    # Generate random data\n+    X = np.random.randn(1000, 5)\n+\n+    # Test without n_init\n+    gm = GaussianMixture(n_components=5)\n+    c1 = gm.fit_predict(X)\n+    c2 = gm.predict(X)\n+    assert_array_equal(c1, c2)\n+\n+    # Test with n_init=5\n+    gm = GaussianMixture(n_components=5, n_init=5)\n+    c1 = gm.fit_predict(X)\n+    c2 = gm.predict(X)\n+\n+    # Assert that the arrays are not equal to reproduce the issue\n+    with pytest.raises(AssertionError):\n+        assert_array_equal(c1, c2)\n\\ No newline at end of file\n",
  "test_inverted_patch": "diff --git a/sklearn/mixture/tests/test_gaussian_mixture_n_init_inverted.py b/sklearn/mixture/tests/test_gaussian_mixture_n_init_inverted.py\nnew file mode 100644\nindex 0000000..a5b45e0\n--- /dev/null\n+++ b/sklearn/mixture/tests/test_gaussian_mixture_n_init_inverted.py\n@@ -0,0 +1,21 @@\n+import numpy as np\n+from sklearn.mixture import GaussianMixture\n+import pytest\n+\n+def test_gaussian_mixture_fit_predict_disagreement():\n+    # Generate random data\n+    X = np.random.randn(1000, 5)\n+\n+    # Test without n_init\n+    gm = GaussianMixture(n_components=5)\n+    c1 = gm.fit_predict(X)\n+    c2 = gm.predict(X)\n+    assert np.array_equal(c1, c2), \"__BUG__HERE__\"\n+\n+    # Test with n_init=5\n+    gm = GaussianMixture(n_components=5, n_init=5)\n+    c1 = gm.fit_predict(X)\n+    c2 = gm.predict(X)\n+\n+    # Assert that the arrays are equal to catch the issue\n+    assert np.array_equal(c1, c2), \"__BUG__HERE__\"\n\\ No newline at end of file\n",
  "num_sent_chars": 156105,
  "num_received_chars": 10507,
  "elapsed_time": 67.83607125282288,
  "lint_repair_count": 0,
  "test_generation_attempts": 0,
  "code_generation_attempts": 1,
  "pass_to_pass": true,
  "pass_to_fail": true,
  "fail_to_pass": true,
  "code_patch_score": 3,
  "appmap_data_test_status": null,
  "appmap_data_file_count": null,
  "appmap_data_context_size": null
}